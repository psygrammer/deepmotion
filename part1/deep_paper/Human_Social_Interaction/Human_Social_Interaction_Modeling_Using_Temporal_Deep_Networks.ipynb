{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Human Social Interaction Modeling Using Temporal Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* ABSTRACT\n",
    "* Keywords\n",
    "* 1 INTRODUCTION\n",
    "* 2 RELATED WORK\n",
    "    - Social Psychology\n",
    "    - Affective Computing\n",
    "    - Hybrid Models\n",
    "    - Deep Networks\n",
    "* 3 APPROACH\n",
    "    - 3.1 Review of Prior Models\n",
    "        - Restricted Boltzmann Machines\n",
    "        - Discriminative Restricted Boltzmann Machines\n",
    "        - Conditional Restricted Boltzmann Machines\n",
    "    - 3.2 Model\n",
    "        - Discriminative Conditional Restricted Boltzmann Machines\n",
    "    - 3.3 Inference and Learning\n",
    "        - Inference\n",
    "        - Learning\n",
    "* 4 EXPERIMENTS\n",
    "    - 4.1 Datasets\n",
    "        - Tower Game Dataset\n",
    "        - Capture Setup\n",
    "        - Data Annotation\n",
    "    - 4.2 Quantitative Results\n",
    "        - Implementation Details\n",
    "        - Results\n",
    "* 5 CONCLUSIONS AND FUTURE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We present a novel approach to computational modeling of social interactions based on modeling of essential social interaction predicates (ESIPs) such as joint attention and entrainment.\n",
    "* Based on sound social psychological theory and methodology, we collect a new “Tower Game” dataset consisting of audio-visual capture of dyadic interactions labeled with the ESIPs.\n",
    "* We propose a novel joint Discriminative Conditional Restricted Boltzmann Machine (DCRBM) model that combines a discriminative component with the generative power of CRBMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hybrid Models\n",
    "* Deep Learning\n",
    "* DCRBMs\n",
    "* Social Interaction\n",
    "* Computational Social Psychology\n",
    "* Tower Game Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This research brings together multiple disciplines to explore the problem of social interaction modeling. \n",
    "* The goal of this work is to leverage research in social psychology, computer vision, signal processing, and machine learning to better understand human social interactions.\n",
    "* we focus on social predicates that support rapport: \n",
    "    - joint attention, \n",
    "    - temporal synchrony, \n",
    "    - mimicry, and \n",
    "    - coordination.\n",
    "* Our approach is guided by two key insights. \n",
    "    - The first is that apart from inferring the mental state of the other, social interactions require individuals to attend each other’s movements, utterances and context to coordinate actions jointly with each other. \n",
    "    - The second insight is that social interactions involve reciprocal acts, joint behaviors along with nested events (e.g. speech, eye gaze, gestures) at various timescales and therefore demand adaptive and cooperative behaviors of their participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESIPs\n",
    "* We focus on detecting \n",
    "    - rhythmic coupling \n",
    "        - (also known as entrainment and attunement), \n",
    "    - mimicry (behavioral matching), \n",
    "    - movement simultaneity, \n",
    "    - kinematic turn taking patterns, and \n",
    "    - other measurable features of engaged social interaction.\n",
    "* We established that behaviors such as <font color=\"blue\">joint attention</font> and <font color=\"blue\">entrainment</font> were the <font color=\"red\">essential predicates of social interaction (ESIPs)</font>.\n",
    "* With this in mind we focus on developing computational models of social interaction, that utilize <font color=\"red\">multimodal sensing</font> and <font color=\"red\">temporal deep learning models</font> <font color=\"blue\">to detect and recognize</font> <font color=\"red\">these ESIPs</font> as well as discover their actionable constituents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### models\n",
    "* Discriminative models \n",
    "    - focus on maximizing the separation between classes, however, they are often uninterpretable. \n",
    "* On the other hand, generative models \n",
    "    - focus solely on modeling distributions and are often unable to incorporate higher level knowledge. \n",
    "* Hybrid models \n",
    "    - tend to address these problems by combining the advantages of discriminative and generative models. \n",
    "    - They encode higher level knowledge as well as model the distribution from a discriminative perspective. \n",
    "* We propose a novel hybrid model that allows us to recognize classes, correlate features, and generate social interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">This paper proposes new approach to machine learning that answers questions posed by social psychology.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 RELATED WORK\n",
    "* Social Psychology\n",
    "* Affective Computing\n",
    "* Hybrid Models\n",
    "* Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Psychology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affective Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 APPROACH\n",
    "* 3.1 Review of Prior Models\n",
    "* 3.2 Model\n",
    "* 3.3 Inference and Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Review of Prior Models\n",
    "* Restricted Boltzmann Machines\n",
    "* Discriminative Restricted Boltzmann Machines\n",
    "* Conditional Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN - http://sanghyukchun.github.io/75/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />\n",
    "<img src=\"figures/cap6.png\" width=600 />\n",
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model\n",
    "* Discriminative Conditional Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Conditional Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Inference and Learning\n",
    "* Inference\n",
    "* Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $⟨·⟩_{data}$ is the expectation with respect to the data distribution and $⟨·⟩_{recon}$ is the expectation with respect to the reconstructed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap24.png\" width=800 />\n",
    "<img src=\"figures/cap25.png\" width=800 />\n",
    "<img src=\"figures/cap26.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 EXPERIMENTS\n",
    "* 4.1 Datasets\n",
    "* 4.2 Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Datasets\n",
    "* Tower Game Dataset\n",
    "* Capture Setup\n",
    "* Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tower Game Dataset\n",
    "* architect-builder\n",
    "* distinct-objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] Tower Game Dataset: A multimodal dataset for analyzing social interaction predicates - http://www.infomus.org/Events/proceedings/ACII2015/papers/Main_Conference/M2_Poster/Poster_Teaser_5/ACII2015_submission_19.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since our focus is on joint attention and entrainment, we annotated 112 videos which were divided into 1213 10-second segments indicating the presence or absence of these two behaviors in each segment.\n",
    "* To annotate the videos, we developed an innovative annotation schema drawn from concepts in the social psychology literature.\n",
    "* Joint attention is the shared focus of two individuals on a common subject and it involves eye gaze (on a person and on an object) and body language.\n",
    "* Entrainment is the alignment in the behavior of two individ- uals and it involves simultaneous movement, tempo similarity, and coordination.\n",
    "* Each measure was rated using a low, medium, high measure for the entire 10 second segment. \n",
    "* We hired six undergraduate sociology and psychology students to annotate the videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Quantitative Results\n",
    "* Implementation Details\n",
    "* Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For our experiments, we relied only on the skeleton features. We use the 11 joints from the upper body of the two players since the tower game almost entirely involves only upper body actions.\n",
    "* Using the 11 joints we extracted a set of first order static and dynamic handcrafted skeleton features.\n",
    "    - The static features \n",
    "        - are computed per frame. \n",
    "        - The features consist of, \n",
    "            - relationships between all pairs of joints of a single actor, \n",
    "                - as well as the relationships between all pairs of joints of both the actors.\n",
    "    - The dynamic features \n",
    "        - are extracted per window (a set of 300 frames). \n",
    "        - In each window, we compute first and second order dynamics (velocities and accelerations) of each joint, \n",
    "            - as well as \n",
    "                - relative velocities and \n",
    "                - accelerations of pairs of joints per \n",
    "                    - actor, and \n",
    "                    - across actors.\n",
    "* The dimensionality of the static and dynamic features is (257400 D). \n",
    "* To reduce their dimensionality we use Principle Component Analysis (PCA) (100 D), Bag-of-Words (BoW) (100 and 300 D). \n",
    "* We also extracted Deep Learning features using RBMs and CRBMs (50 dimensions)\n",
    "* For the DRBM and DCRBM we used the raw joint locations normalized with respect to a selected origin point. We used the same dimensionality for both models D(v) = 66,D(h) = 50. \n",
    "* For DCRBM we empirically evaluated history windows of different sizes, and found that a window of size n = 15 works the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this paper we focused on the three ECIPs,\n",
    "* Coordination, \n",
    "* Simultaneous Movement, and \n",
    "* Tempo Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation is done with respect to the six annotators {$A_1,A_2,...,A_6$} as well as the mean annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap20.png\" width=600 />\n",
    "<img src=\"figures/cap21.png\" width=600 />\n",
    "<img src=\"figures/cap22.png\" width=600 />\n",
    "<img src=\"figures/cap23.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The second task is the Generation Task, where we are given the class label and our goal is to generate the data (i.e. the raw features) for that label. \n",
    "    - This task allows us to visualize what the classifier has learned.\n",
    "* For generation, we initialize the model using 15 frames for each person, and then generate sequences of lengths varying from 16 to 300 frames.\n",
    "* We measure the mean error between the groundtruth data and the generated data for each class label over 50 video instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap27.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation is done in two different settings.\n",
    "* In the first setting, \n",
    "    - given partial visible player data (one player’s features) as well as the class label, \n",
    "    - the goal is to generate the other player’s data.\n",
    "* In the second setting, \n",
    "    - given only the class label, \n",
    "    - the goal is to generate the entire visible layer data (i.e. the raw features for both the players)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap24.png\" width=800 />\n",
    "<img src=\"figures/cap25.png\" width=800 />\n",
    "<img src=\"figures/cap26.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 CONCLUSIONS AND FUTURE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 \n",
    "* [1] Human Social Interaction Modeling Using Temporal Deep Networks - https://arxiv.org/abs/1505.02137\n",
    "* [2] Machine Learning 스터디 (19) Deep Learning - RBM, DBN, CNN - http://sanghyukchun.github.io/75/\n",
    "* [3] Tower Game Dataset: A multimodal dataset for analyzing social interaction predicates - http://www.infomus.org/Events/proceedings/ACII2015/papers/Main_Conference/M2_Poster/Poster_Teaser_5/ACII2015_submission_19.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
